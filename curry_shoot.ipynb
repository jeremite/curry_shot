{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_wm_username = 'wli10'\n",
    "search_url = 'http://buckets.peterbeshai.com/api/?player=201939&season=2014'\n",
    "response = requests.get(search_url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\"})\n",
    "\n",
    "url = 'http://buckets.peterbeshai.com/api/?player=201939&season=2015'\n",
    "data = requests.get(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\"})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "cor =[]    \n",
    "made = []\n",
    "\n",
    "for shot in response.json():\n",
    "    cor.append((shot['LOC_X'],shot['LOC_Y']))\n",
    "    made.append(shot['EVENT_TYPE'])\n",
    "    \n",
    "\n",
    "for shots in data.json():\n",
    "    cor.append((shots['LOC_X'],shots['LOC_Y']))\n",
    "    made.append(shots['EVENT_TYPE'])\n",
    "      \n",
    "len(cor),len(made)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(made)):\n",
    "    if made[i] == 'Made Shot':\n",
    "        made[i] =1\n",
    "    else:\n",
    "        made[i] =0\n",
    "len(made)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in cor:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.array(list(val)) for val in cor]\n",
    "x = np.array(x).T\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(made,dtype=np.uint8).reshape(-1,len(made))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.shape[1]*3//4\n",
    "X_train = x[:,0:n]/712\n",
    "X_test = x[:,n:]/712\n",
    "y_train = y[:,0:n]\n",
    "y_test = y[:,n:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, cor[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.max(),X_train.min(), y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0, 0:211].shape,X_train[1, 0:211].shape,y_train[:,0:211].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_train[:,0:211]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[0, 0:211], X_train[1, 0:211],  c = y_new, s=40, cmap=plt.cm.Spectral)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    s = 1 / (1+np.exp(-X))\n",
    "    return s\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    s = np.maximum(0,X)\n",
    "    return s\n",
    "\n",
    "def tanh(X):\n",
    "    s = (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "    return s\n",
    "    \n",
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for i in range(1,L):\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1])\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i],1))\n",
    "        \n",
    "        assert(parameters['W' + str(i)].shape == (layer_dims[i], layer_dims[i-1]))\n",
    "        assert(parameters['b' + str(i)].shape == (layer_dims[i], 1))        \n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "def forward_prop(X,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    \n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    \n",
    "    \n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = tanh(Z2)\n",
    "    Z3 = np.dot(W3,A2)+b3\n",
    "    A3 = tanh(Z3)\n",
    "    \n",
    "    Z4 = np.dot(W4,A3)+b4\n",
    "    A4 = sigmoid(Z4)\n",
    "    \n",
    "    cache = (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3,Z4,A4,W4,b4)\n",
    "    return A4,cache\n",
    "\n",
    "\n",
    "def backward_prop(X,y,cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3,Z4,A4,W4,b4) = cache\n",
    "    \n",
    "    dZ4 = A4-y\n",
    "    dW4 = 1./m * np.dot(dZ4,A3.T)\n",
    "    db4 = 1./m * np.sum(dZ4, axis =1, keepdims = True)\n",
    "    \n",
    "    dA3 = np.dot(W4.T,dZ4)\n",
    "    dZ3 = np.multiply(dA3 ,(1-A3**2))\n",
    "    dW3 = 1. /m * np.dot(dZ3,A2.T)\n",
    "    db3 = 1. /m * np.sum(dZ3,axis=1,keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 = dA2 * (1-A2**2)\n",
    "    dW2 = 1. /m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1. /m * np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = dA1 * (1-A1**2)\n",
    "    dW1 = 1. /m * np.dot(dZ1,X.T)\n",
    "    db1 = 1. /m * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ4\": dZ4, \"dW4\": dW4, \"db4\": db4,\n",
    "                 \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_para(gradients,parameters,learning_rate):\n",
    "    n = len(parameters) //2\n",
    "    for i in range(n):\n",
    "        parameters['W'+str(i+1)] = parameters['W'+str(i+1)] - learning_rate*gradients['dW'+str(i+1)]\n",
    "        parameters['b'+str(i+1)] = parameters['b'+str(i+1)] - learning_rate*gradients['db'+str(i+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X,y,parameters):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A4,cache = forward_prop(X,parameters)\n",
    "    \n",
    "    p = np.zeros((1,m))\n",
    "    for i in range(0,A4.shape[1]):\n",
    "        if A4[0,i] > 0.5:\n",
    "            p[0,i] =1\n",
    "        else:\n",
    "            p[0,i] =0\n",
    "    print('accuracy rate is: ',np.mean((p[0,:]==y[0,:])))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def compute_cost(A4,y):\n",
    "    m = y.shape[1]\n",
    "    loss = -(y*np.log(A4)+(1-y)*np.log(1-A4))\n",
    "    cost = 1./m * np.nansum(loss)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def get_batch(X,y,batch_size = 16):\n",
    "    m = X.shape[1]\n",
    "    perm = np.random.permutation(m)\n",
    "    X_perm = X[:,perm]\n",
    "    y_perm = y[:,perm]\n",
    "    \n",
    "    n = np.int(m/batch_size)\n",
    "    batches = []\n",
    "    for i in range(n):\n",
    "        X_batch = X_perm[:,i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y_perm[:,i*batch_size:(i+1)*batch_size]\n",
    "        batches.append((X_batch,y_batch))\n",
    "    if m%batch_size != 0:\n",
    "        X_batch = X_perm[:,n*batch_size:]\n",
    "        y_batch = y_perm[:,n*batch_size:]       \n",
    "        batches.append((X_batch,y_batch))\n",
    "    return batches\n",
    "    \n",
    "def predict_dec(X,parameters):\n",
    "    a4, cache = forward_prop(X,parameters)\n",
    "    predictions = (a4>0.5)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int(32/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims =[X.shape[0],32,20,3,1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30001, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    layer_dims =[X.shape[0],32,20,3,1]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    \n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob ==1:\n",
    "            A4,cache = forward_prop(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            A4,cache = forward_prop_with_dropout(X, parameters)\n",
    "            \n",
    "        # cost function\n",
    "     \n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(A4, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(A4, Y, parameters, lambd)\n",
    "            \n",
    "               # Backward propagation.\n",
    "        assert(lambd==0 or keep_prob==1) \n",
    "        \n",
    "        \n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_prop(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)        \n",
    "   \n",
    "                \n",
    "       # grads = backward_propagation(X, Y, cache)\n",
    "        parameters = update_para(grads,parameters,learning_rate)\n",
    "    \n",
    "        if print_cost and i%10000 == 0:\n",
    "            print('Cost after iteration {}: {}'.format(i,cost))\n",
    "        if print_cost and i%1000 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('cost')\n",
    "    plt.ylabel('iteration (X1000)')\n",
    "    plt.title('Learning rate = ' +str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pure(X, Y, learning_rate = 0.3, num_iterations = 3000, print_cost = True):\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    layer_dims =[X.shape[0],32,20,3,1]\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    \n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        batches = get_batch(X,Y,batch_size = 32)\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        for b in range(len(batches)):\n",
    "            X_batch, y_batch = batches[b]\n",
    "            A4,cache = forward_prop(X_batch, parameters)\n",
    "            cost = compute_cost(A4, y_batch)\n",
    "            grads = backward_prop(X_batch, y_batch, cache)\n",
    "            parameters = update_para(grads,parameters,learning_rate)\n",
    "    \n",
    "        if print_cost and i%1000 == 0:\n",
    "            print('Cost after iteration {}: {}'.format(i,cost))\n",
    "        if print_cost and i%100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('cost')\n",
    "    plt.ylabel('iteration (X1000)')\n",
    "    plt.title('Learning rate = ' +str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model_pure(X_train, y_train,learning_rate = 0.003)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(X_train, y_train, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(128,activation='tanh')(X_input)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(64,activation='tanh')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(32,activation='tanh')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(32,activation='tanh')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(16,activation='tanh')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(8,activation='tanh')(X)\n",
    "    X = BatchNormalization(axis=-1)(X)\n",
    "    X = Dense(1,activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input,outputs=X)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_model = model((2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.T\n",
    "Y = y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_model.fit(X,Y,epochs=30,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
